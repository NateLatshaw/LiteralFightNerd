[
  {
    "path": "posts/2021-02-25-extending-judgeai/",
    "title": "Extending JudgeAI",
    "description": "Aggregating JudgeAI's round score predictions to generate final scorecard probability distributions with a case study on the judging decisions made during the main event of UFC 247: Jones vs Reyes",
    "author": [
      {
        "name": "Nate Latshaw",
        "url": {}
      }
    ],
    "date": "2021-02-25",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nCase study: Jon Jones vs Dominick Reyes\r\nRound 1\r\nRound 2\r\nRound 3\r\nRound 4\r\nRound 5\r\n\r\nMethodology: computing final scorecard probabilities\r\nJones vs Reyes final scorecard\r\nConclusion\r\n\r\nIntroduction\r\nIn my previous blog post, I introduced JudgeAI, a machine learning algorithm that uses round-level statistics to predict judges’ scores by round in the UFC. That post illustrated in detail how JudgeAI was built and how it performs across time. Overall, the model provides well-calibrated round win probabilities and is able to predict round scores with high accuracy. I strongly suggest reading that post if you are interested in understanding the methodological details of JudgeAI. Alternatively, if you’re interested in a quicker summary of the details of an earlier version of the model, you can check out my posts on Reddit and Twitter - those social media posts combine the information presented in this blog post and my previous post. This post largely assumes you either understand the technical details of JudgeAI or do not care about such details - either is perfectly fine.\r\nThis post seeks to extend JudgeAI by leveraging its round score probabilities to produce a probability distribution of all possible final scores. I hope audiences of all types find this post approachable. We explore the methodology by taking a look at the Jon Jones vs Dominick Reyes fight from UFC 247 on February 8, 2020, which was a unanimous, but controversial, decision by the judges. I also include a brief methodology section for those interested in how the round score probabilities are aggregated to produce final scorecard predictions.\r\nAs we will see, using JudgeAI’s round score probabilities to generate a distribution of final scores is a powerful way to utilize the underlying uncertainty in the round-level scoring, and then propagate that uncertainty across rounds, to understand how likely or unlikely each possible final score is. It also allows us to move away from thinking about discrete scores and move towards thinking probabilistically about judging decisions.\r\nCase study: Jon Jones vs Dominick Reyes\r\nRound 1\r\nThe figure below shows a number of statistics from the first round of Jones vs Reyes. Note that JudgeAI includes many more features than those shown here. The figure also displays the score given by each judge, the score given by JudgeAI, and JudgeAI’s probability of each score in 10-9, 10-8, 9-10, and 8-10.\r\nWe see that Reyes landed more significant strikes and only absorbed a single strike to the head. All 3 judges and JudgeAI scored this round 9-10 in favor of Reyes. Also, JudgeAI shows high confidence in its predicted score by placing an 81% probability on the majority of judges giving that score.\r\n\r\n\r\n\r\nRound 2\r\nThe figure below shows round 2. We see that Reyes landed 50% more significant strikes than Jones, and he landed more strikes to each of the head, body, and legs. JudgeAI again scored this 9-10 to Reyes. However, 2 of the 3 judges scored the round 10-9 to Jones.\r\nIt is important to note here that JudgeAI is not perfect. The model identifies broad trends in the relationship between round-level statistics and judges’ scores. Here the model is essentially saying that rounds with these striking differentials (plus all other statistics considered but not shown) go to the fighter who landed more strikes, which is Reyes here, around 3 out of every 4 times. While JudgeAI placed a high probability on Reyes winning this round, we cannot say for sure that 2 of the judges got this round wrong. Though striking differentials often correlate with damage dealt, for example, the model does not perfectly account for this. It is possible that Jones dealt more damage even though he landed fewer strikes.\r\nFinally, notice that JudgeAI was less certain about this 9-10 score than in the previous round. There is uncertainty in round-level scoring because judges are subjective and see different things in each round. This uncertainty propagates across rounds and can result in seemingly strange final scores. Hence, it is more important to focus on JudgeAI’s score probabilities than its predicted score of 9-10.\r\n\r\n\r\n\r\nRound 3\r\nRound 3 scores and statistics are displayed in the figure below. We see that Reyes again landed more significant strikes than Jones, and again, JudgeAI scored this round 9-10 to Reyes. However, 2 of the 3 judges scored this round 10-9 to Jones.\r\nOver the last 2 rounds, either JudgeAI has been missing some important factor in the fight, like damage dealt discrepancies, or it is safe to say that the judges are not doing Reyes any favors. The judges have been split on the last 2 rounds, which suggests these rounds have been tight, even though Reyes has had the striking advantage on paper.\r\n\r\n\r\n\r\nRound 4\r\nThe figure below shows round 4. We see that Jones landed more strikes than Reyes overall and to each of the head, body and legs. In addition, Jones landed the only takedown of the round and accumulated nearly 1 minute of control time. JudgeAI scored this round 10-9 to Jones with a very high probability, and all 3 judges agreed.\r\n\r\n\r\n\r\nRound 5\r\nThe final round is displayed below. Jones landed more significant strikes than Reyes and secured a takedown with around 30 seconds of control time. As in the last round, JudgeAI scored this round 10-9 to Jones with a high probability, and all 3 judges agreed.\r\n\r\n\r\n\r\nMethodology: computing final scorecard probabilities\r\nThis section provides a quick overview of the methodological details involved in computing the forthcoming final score probabilities. If that is not something that interests you, feel free to skip to the next section.\r\nRecall that our goal is to utilize JudgeAI’s round score probabilities to produce a probability distribution of all possible final scores. Such a distribution will provide more information than a discrete final score, like 48-47. Instead, the distribution will show how likely or unlikely each final score is after accounting for the uncertainty in round scoring that propagates across rounds.\r\nSince JudgeAI returns predicted probabilities for each score in 10-9, 10-8, 9-10, and 8-10, mathematically deriving the probability of each possible final score after 3 or 5 rounds is cumbersome. Hence, we instead use a Monte Carlo method to numerically compute final scorecard probabilities. That is, we essentially simulate the scoring of each round many times using JudgeAI’s round score probabilities and then see what proportion of simulated fights had each final score.\r\nThe computational algorithm employed is below.\r\nDraw 1 sample score for each round using JudgeAI’s corresponding round score probabilities. That is, for each round, draw a sample score from the possible scores 10-9, 10-8, 9-10, and 8-10 where the probability of drawing each possible score is set to the corresponding round score probability provided by JudgeAI.\r\nAdd each fighter’s sample scores together across rounds to generate the sample final score for the fight.\r\nRepeat steps 1-2 100,000 times. This yields 100,000 final score samples.\r\nCompute final scorecard probabilities by dividing the number of times each final score appears in the samples by 100,000.\r\nJones vs Reyes final scorecard\r\nThe figure below shows the final scorecard for the fight and, according to JudgeAI coupled with the Monte Carlo method outlined above, the probability of each possible final score. We see that all 3 judges had Jones winning. However, if we add up JudgeAI’s predicted scores by round, we find that JudgeAI scored the fight 47-48 in favor of Reyes. Without properly aggregating JudgeAI’s round score probabilities, our analysis would end here, but fortunately, we recover a probability distribution of final scores that enables much more in-depth analysis.\r\nUsing the aforementioned algorithm to aggregate round score probabilities, we see that the most likely final score of 47-48 in favor of Reyes is expected to occur with a 43% probability. In other words, JudgeAI predicts that over many fights with the exact same round-level statistics as this fight, Reyes would win the decision 47-48 around 43% of the time.\r\nFurther, Reyes’ win probability (computed by adding up the appropriate final score probabilities) was 55%. Again, this means that over many fights with identical round-level statistics, JudgeAI predicts that Reyes would win via decision more often than not.\r\n\r\n\r\n\r\nEven though JudgeAI has Reyes winning this fight, notice that the win probability was barely over 50%. According to JudgeAI, the two most dominant rounds were 4 and 5, both of which went to Jones. This means that JudgeAI was more certain that the judges would also give these rounds to Jones, which turned out to be true as Jones won those rounds 10-9 on every scorecard. Since rounds are scored independently, once a fighter in a 5 round fight secures 2 dominant rounds, the math begins moving in that fighter’s direction. As we saw, JudgeAI had Reyes winning the other 3 rounds, which should be enough to win the decision. However, since Reyes’ round wins (according to JudgeAI) were less dominant, it was more likely that some judges would award some of those rounds to Jones, which is also what we saw.\r\nThis is the difference between using JudgeAI to score rounds probabilistically vs keeping track of one’s own personal round scores at home. The latter can lead to confusion or anger when comparing one’s own final score to that of the judges, while the latter is more likely to help us understand how likely or unlikely a particular judge’s final score was. In this case, according to JudgeAI, two judges scoring the fight 48-47 to Jones was not terribly egregious, but the 49-46 score to Jones was relatively unlikely.\r\nIn the end, after accounting for the round-level scoring uncertainty, JudgeAI had Reyes winning with a 55% probability, which is quite low coming from a model that had Reyes winning 3 rounds. In this case, the uncertainty captured by JudgeAI lowered Reyes’ win probability from what we might have expected and happened to coincide with Jones winning the fight on each judge’s scorecard. Though JudgeAI still disagrees with the judges on the winner of the fight, the model only awarded Reyes a 55% win probability, which makes this fight a toss up.\r\nWhile the judges all agreed that Jones won the fight and JudgeAI had the fight as a toss up, the community seems to feel much more strongly that Reyes won. According to MMADecisions, which posts the official scorecards along with scores provided by members of the media and by fans, 14 of the 21 media members scored the fight 47-48 in favor of Reyes. The other 7 members of the media scored it 48-47 to Jones. In addition, as of the writing of this post, 77% of the fans who submitted scores had Reyes winning, the majority of which scored it 47-48 to Reyes. The scores provided by the community align with the two most likely final scores according to JudgeAI, but the share of media members and fans awarding the fight to Reyes is much higher than JudgeAI’s win probability for Reyes. Thus, the actual final scorecard submitted by the judges was generally much more surprising to media members and fans than it was to JudgeAI, which is likely due to the probabilistic decision-making of JudgeAI that is typically challenging for humans to replicate on their own.\r\nConclusion\r\nThe purpose of this post was not to argue that Reyes should have won the Lightweight title against Jones at UFC 247. Instead, this post serves to illustrate how the round score probabilities provided by JudgeAI can be aggregated to produce a probability distribution of final scores that is both unique and insightful.\r\nWe saw how using JudgeAI’s discrete round scores to score the fight resulted in a final score that did not match any of the judges on the final scorecard. However, leveraging JudgeAI’s round score probabilities allowed us to think probabilistically about how likely each final score might be. That is, we saw that Jones won the two most dominant rounds and thus only needed to win 1 additional round among the 3 more competitive rounds. Hence, even though JudgeAI had Reyes winning the 3 more contested rounds, it only gave Reyes a 55% chance of winning the decision because of all the uncertainty in how judges were likely to score those 3 rounds. Media members and fans had Reyes winning at significantly higher rates than JudgeAI. While the resulting frustration among members of the media and fans may or may not be justified, I think the probabilistic features of scoring a fight with JudgeAI provides the community with valuable insights that are tough to get anywhere else.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-25-extending-judgeai/extending-judgeai_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-02-25T23:23:56-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-02-19-introducing-judgeai/",
    "title": "Introducing JudgeAI",
    "description": "A machine learning algorithm that predicts judges' scores by round in the UFC",
    "author": [
      {
        "name": "Nate Latshaw",
        "url": {}
      }
    ],
    "date": "2021-02-19",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nUFC data exploration\r\nBuilding JudgeAI\r\nEvaluating JudgeAI\r\nLimitations of JudgeAI\r\nExtending JudgeAI\r\n\r\nIntroduction\r\nFights in the Ultimate Fighting Championship (UFC) consist of 3 or 5 rounds, each of which lasts 5 minutes in duration. At any point in any round, the fight may end via knockout or submission. Otherwise, the winner of the fight is determined by the judges at the end of the final round. Each round is scored independently by 3 judges based on effective striking, effective grappling, octagon control, aggressiveness, and effective defense - in that order. Each judge awards 10 points to the winner of each round and 9 or fewer points to the loser (with some incredibly rare exceptions). Then, after the final round, each judge’s scores are added across rounds to arrive at the final scorecard. The fighter with more points for the majority of final scores is the winner of the fight.\r\nJudging decisions can be controversial. It’s often unclear how a particular judge came up with a certain seemingly unlikely score. Enter JudgeAI, a machine learning model that predicts round scores. If successful, JudgeAI will identify trends in the relationship between striking/grappling statistics and round scores given by the judges in order to quantify how likely or unlikely each possible score is for a round with a given set of statistics. While the publicly available statistics are far from a perfect measure of what the judges can see with their eyes throughout each round, it is my hope that these statistics are enough to proxy for the criteria judges use to score rounds such that JudgeAI need not watch a fight in order to accurately predict round scores.\r\nThe purpose of this post is twofold:\r\nCreate a machine learning model, JudgeAI, that utilizes publicly available round-level statistics and judging decisions to predict round scores.\r\nDevelop a methodologically sound framework to properly illustrate how JudgeAI would have performed had it been implemented in the past in order to provide a valid measure of how well the model will perform in the future.\r\nOne final note before we get started: this post is primarily methodological in nature, which is admittedly off-putting to some. This post serves to illustrate what I did, how I did it, and how good or bad JudgeAI performs. I plan to use this post and the methodology described herein as a foundation for a number of derivative results and metrics that are hopefully much more accessible to those who enjoy the UFC but are not interested in these methodological details. It is my hope that this post will be a comprehensive reference for anyone interested in the details of JudgeAI.\r\nUFC data exploration\r\nAccording to official UFC statistics, there were 4,221 fights in the UFC between 2011-2020, 48.4% of which resulted in a winner being declared by the judges’ scorecard. The figure below shows how UFC fights ended over this period. This post will focus on split and unanimous decisions only. A split decision occurs when the 3 judges do not all agree on which fighter won the fight, while a unanimous decision occurs when all 3 judges have the same winner on the final scorecard. Fortunately, the figure below shows that the number of split decisions is not noticeably rising over the past few years. Instead, if anything, judges are agreeing with each other at higher rates at the end of the decade, as measured by the proportion of decisions that are unanimous.\r\n\r\n\r\n\r\nUsing data from http://www.mmadecisions.com/, we explore how judges score fights by round. However, we first have to combine the judging data with the official UFC statistics data. As a result, we lose some fights. First, we drop all fights that do not end in a decision by the judges since scorecards are not publicly reported for any of the rounds in these fights. Then, we drop rounds where judging data was otherwise not available or where judges gave scores other than 10-8, 10-9, 9-10, or 8-10 since these potentially indicate a penalty where a point was deducted from one of the fighters (or other rare edge cases). By the end, of the 2,045 UFC fights between 2011-2020 where a winner was declared by the judges, 1,670 fights (containing 5,238 rounds) are successfully merged with the judging decisions data and retained for analysis. Unless otherwise stated, all subsequent counts and statistics refer to this set of fights, which comprises 81.7% of fights in which the judges selected a winner and 39.6% of all UFC fights between 2011-2020.\r\nIn order to predict round scores, we need to determine the “ground truth” score for each round. This is an inherently impossible task because if we had the ground truth then we would not need judges. Hence, we consider the “true” score of the round to be the score given by the majority of judges (i.e. at least 2 out of the 3 judges). Of the 5,238 rounds under consideration, all 3 judges provided the same score for 76.9%, or 4,029 rounds.\r\nIn the UFC, fighters are assigned to either the red corner or the blue corner. For title fights, the defending champion is always the red corner and the challenger is always the blue corner. For all other fights, the reasoning behind the corner assignments are not quite clear (at least not to me), but the red corner generally appears to be the more well-known and/or more experienced fighter. The figure below shows how rounds were scored between 2011-2020 (using the “true” round score), where the reported scores follow the convention: Red Corner Score - Blue Corner Score.\r\nWe see in the figure that red corners win more often than blue corners in each year. Over the entire period, the red corner won 57.6% of all rounds considered. Also, we notice that 10-8 and 8-10 rounds are incredibly rare, comprising only 3.3% of all rounds considered.\r\n\r\n\r\n\r\n\r\n\r\n\r\nRecall that our goal is to develop a machine learning model that predicts judges’ scores using the statistics accumulated in each round. However, before we do that, we first consider a simple, baseline decision rule that predicts round winners to which we can compare our eventual model’s performance. That is, the baseline decision rule predicts that whichever fighter landed more significant strikes in a round is the winner. Among rounds in which each fighter landed the same number of significant strikes, the fighter who landed more total strikes is predicted to be the winner. Finally, among the rounds remaining where each fighter landed the same number of significant and total strikes (only 0.9% of all rounds considered), the red corner is predicted to be the winner.\r\nTable 1 below shows how well our baseline decision rule is able to predict round winners across time. We see that the baseline rule performs relatively well in each year, which makes sense since the baseline only considers striking and since effective striking is the metric given the highest priority in the judges’ decision making. Of all rounds considered, the baseline rule correctly predicts the winner 77.8% of the time.\r\n\r\n\r\nTable 1: Baseline Round Winner Accuracy by Year\r\n\r\n\r\nYear\r\n\r\n\r\nNumber of Rounds\r\n\r\n\r\nBaseline Round Winner Accuracy\r\n\r\n\r\n2011\r\n\r\n\r\n237\r\n\r\n\r\n79.3%\r\n\r\n\r\n2012\r\n\r\n\r\n353\r\n\r\n\r\n80.5%\r\n\r\n\r\n2013\r\n\r\n\r\n348\r\n\r\n\r\n74.7%\r\n\r\n\r\n2014\r\n\r\n\r\n533\r\n\r\n\r\n76.9%\r\n\r\n\r\n2015\r\n\r\n\r\n604\r\n\r\n\r\n78.5%\r\n\r\n\r\n2016\r\n\r\n\r\n584\r\n\r\n\r\n75.0%\r\n\r\n\r\n2017\r\n\r\n\r\n562\r\n\r\n\r\n78.5%\r\n\r\n\r\n2018\r\n\r\n\r\n556\r\n\r\n\r\n79.1%\r\n\r\n\r\n2019\r\n\r\n\r\n766\r\n\r\n\r\n77.3%\r\n\r\n\r\n2020\r\n\r\n\r\n695\r\n\r\n\r\n78.8%\r\n\r\n\r\nNote: \r\n\r\n\r\n -Baseline decision rule predicts the winner as the fighter who landed more significant strikes. Ties go to whoever landed more total strikes, or the red corner if total strikes are also even.-“Round Winner Accuracy” is defined as the share of rounds in which the predicted round winner matches the winner declared by the majority of judges.\r\n\r\n\r\nNow that we have established a baseline decision rule, we proceed with building the predictive model, JudgeAI. The baseline decision rule only considers strikes landed, while the actual model will consider many more features for each round. If the eventual predictive model is any good, it will predict round winners more successfully than this simple decision rule.\r\nBuilding JudgeAI\r\nThe outcome of interest for JudgeAI is the score given by the majority of judges for each round. That is, the outcome will be one of 10-9, 10-8, 9-10, or 8-10, and again, the scores follow the convention: Red Corner Score - Blue Corner Score. JudgeAI will use the statistics accumulated by each fighter during each round to predict the outcome. The official UFC statistics provide striking and grappling statistics, to include total strikes landed and attempted, significant strikes landed and attempted, knockdowns, takedowns landed and attempted, submission attempts, reversals, and control time. In addition, significant strikes are broken down into strikes to the head, body, or legs; and strikes at distance, in the clinch, or on the ground. Using strikes and takedowns landed and attempted, we also compute striking and takedown accuracy, respectively. Finally, rather than including each fighter’s round-level statistics separately in the model, we instead include the difference by taking the red corner’s statistic minus that of the blue corner, for each statistic included. In total, there are 31 features included in the model, all of which are statistical differences.\r\nPut more simply, each observation in the model is a single round where the outcome is the score given by the majority of judges, and the features used to predict each majority score are the differences between each fighter’s accumulated statistics. For example, one feature in the model is the difference in total strikes landed for each round, which is computed by subtracting the number of total strikes landed by the blue corner from the number of total strikes landed by the red corner.\r\nSince we are predicting scores over time and since judging and fighter tendencies can change over time, we employ time series cross validation to train and evaluate our predictions. That is, for each month in the period, beginning with January 2012, we train the model using data from all prior months in the period and then make predictions on the single month under consideration. Table 2 below illustrates the time series cross validation strategy where a series of models is trained, each of which predicts scores for a single month of fights (“Evaluation Month” from the table) using all prior data in the period (“Training Months” from the table). Using time series cross validation, we obtain predictions for every month in the “Evaluation Month” column of the table below.\r\n\r\n\r\nTable 2: Time Series Cross Validation Strategy\r\n\r\n\r\nTraining Months\r\n\r\n\r\nEvaluation Month\r\n\r\n\r\nJanuary 2011 - December 2011\r\n\r\n\r\nJanuary 2012\r\n\r\n\r\nJanuary 2011 - January 2012\r\n\r\n\r\nFebruary 2012\r\n\r\n\r\nJanuary 2011 - February 2012\r\n\r\n\r\nMarch 2012\r\n\r\n\r\n…\r\n\r\n\r\n…\r\n\r\n\r\nJanuary 2011 - October 2020\r\n\r\n\r\nNovember 2020\r\n\r\n\r\nJanuary 2011 - November 2020\r\n\r\n\r\nDecember 2020\r\n\r\n\r\nNote: \r\n\r\n\r\n -Data in the Training Months are used to build each model. Predictions are then made on data in the “Evaluation Month”.\r\n\r\n\r\nThis modeling strategy contains 3 primary advantages over a more simple approach of splitting the data into a training and test set - these advantages are:\r\nAll predictions are made on fights unseen by the model through which predictions are made (i.e. we use out-of-sample predictions only).\r\nWe recover predictions for the entire period, except for fights in 2011. Since 2011 is the first year we have data, we only use it for training.\r\nWe are not “cheating” by using a model trained on fights that come after the fights on which we are predicting. This more accurately simulates how successful JudgeAI would have performed had it been implemented in the past, and it gives us a better sense of how well JudgeAI will perform going forward.\r\nThese 3 advantages are significant and essential to the validity of the model evaluation section. Without (1), we would be making predictions on rounds that the model has already seen, which would incorrectly inflate the performance. Further, advantage (2) is important because it provides us with a large sample by which to evaluate model performance, which provides us with a more robust estimate of future performance. Finally, without (3), the model would not accurately convey historical performance. For example, if the model is trying to predict round scores for fights in 2014 but is trained using rounds from 2018, the resulting performance would not be realistic because the model is using fights that have not happened yet to make predictions.\r\nThe algorithm I’ve chosen to use is random forests since it quickly and efficiently builds an ensemble of decision trees to make predictions without overfitting. Also, decision trees are able to capture complex interactions between features, which means we do not have to specify which combinations of features should be considered in conjunction with one another; instead, the algorithm will detect these relationship automatically by design. Each algorithm in the time series cross validation is trained using 500 decision trees to grow a probability forest. Hence, for each round, the algorithm provides a predicted probability of each possible score out of the scores 10-9, 10-8, 9-10, and 8-10.\r\n\r\n\r\n\r\nEvaluating JudgeAI\r\nAfter performing time series cross validation, we now have a predicted probability of each possible score for each round in our sample (again, excluding fights in 2011). After aggregating these round score probabilities up to round win probabilities, the figure below shows how well these round win probabilities are calibrated. If JudgeAI yields well-calibrated probabilities, then we should observe, for example, the red corner winning approximately 60% of rounds in which JudgeAI gives the red corner around a 60% probability of winning. In other words, the figure below checks whether the round win probabilities mean what we want them to mean. Each point represents a group of rounds with both (a) red corner predicted win probabilities centered around the corresponding points on the horizontal axis, and (b) observed red corner win shares equal to the corresponding points along the vertical axis. Points that hug the displayed 45 degree line are considered well calibrated. Since this is real-world data, perfect calibration is not expected, and overall, this figure is quite promising. In aggregate, JudgeAI’s predicted round win probabilities do more or less reflect the share of rounds won historically.\r\n\r\n\r\n\r\nUsing the predicted probabilities for each possible score, we define the predicted score as the score with the highest predicted probability from the model. We then compute the scoring accuracy of the model by determining the number of rounds in which the predicted score matches the score given by the majority of the judges and then dividing by the total number of rounds. That is,\r\n\\(\\text{Scoring accuracy} = \\frac{\\text{# of rounds where predicted score = judges' majority score}}{\\text{total number of rounds}}\\).\r\nOver all fights considered between 2012-2020, the model’s scoring accuracy is 80.2%, which means that the model agrees with the majority of judges on the particular score in approximately 4 out of every 5 rounds.\r\nDiving deeper into performance metrics, the model predicts the same round winner as the majority of judges in 83.3% of rounds. An example of the difference between this measure and the scoring accuracy reported before would be a round where the model’s predicted score is 10-9, but the majority of judges score it 10-8. Both the model and the judges have the same round winner, but they disagree on the particular score of the round.\r\nWhile JudgeAI performs well based on how frequently it agrees with the majority of judges, it is also beneficial to see how often it agrees with any judge. If all 3 judges agree on a particular round score or round winner, it is much more likely that this score or winner is objectively correct. Hence, JudgeAI should very rarely give a predicted score or predicted round winner that disagrees with every judge. Fortunately, such occurrences are rare. The model’s predicted scores match at least 1 judge’s score in 90.8% of rounds. Likewise, the model’s predicted round winner matches that of at least 1 judge in 92.3% of rounds.\r\nFurther, the figure below displays the model’s performance by year and shows that performance is both strong and consistent across time. To the extent that judging biases or preferences change over time, the model adapts well without any egregious degradation. Similarly, as the sport of mixed martial arts (MMA) evolves over time, the judges may modify their internal interpretations of the scoring criteria in order to consistently and fairly score fights. Based on the consistency of the model’s performance over time, the evolution of the sport across the period does not appear to adversely impact the generalizability of the model.\r\n\r\n\r\n\r\nTable 3 below compares the model’s performance to the aforementioned baseline decision rule with respect to round winner accuracy. In each year considered, the model is more accurate at determining round winners, which is expected since it accounts for more than just strikes landed. Moreover, the model is significantly more valuable than the baseline because it makes predictions for each possible round score, instead of only the round winner, and it assigns a probability to each possible outcome, instead of just a binary prediction. Ultimately, the baseline decision rule is not a tough bar to clear, but it is nonetheless encouraging that our model is a drastic improvement over a hypothetical judge that only considers strikes landed and ignores all other aspects of the fight.\r\n\r\n\r\nTable 3: Comparing Round Winner Predictions by Year\r\n\r\n\r\nYear\r\n\r\n\r\nNumber of Rounds\r\n\r\n\r\nBaseline Round Winner Accuracy\r\n\r\n\r\nJudgeAI Round Winner Accuracy\r\n\r\n\r\n2012\r\n\r\n\r\n353\r\n\r\n\r\n80.5%\r\n\r\n\r\n83.0%\r\n\r\n\r\n2013\r\n\r\n\r\n348\r\n\r\n\r\n74.7%\r\n\r\n\r\n83.3%\r\n\r\n\r\n2014\r\n\r\n\r\n533\r\n\r\n\r\n76.9%\r\n\r\n\r\n84.2%\r\n\r\n\r\n2015\r\n\r\n\r\n604\r\n\r\n\r\n78.5%\r\n\r\n\r\n84.1%\r\n\r\n\r\n2016\r\n\r\n\r\n584\r\n\r\n\r\n75.0%\r\n\r\n\r\n81.3%\r\n\r\n\r\n2017\r\n\r\n\r\n562\r\n\r\n\r\n78.5%\r\n\r\n\r\n83.5%\r\n\r\n\r\n2018\r\n\r\n\r\n556\r\n\r\n\r\n79.1%\r\n\r\n\r\n84.5%\r\n\r\n\r\n2019\r\n\r\n\r\n766\r\n\r\n\r\n77.3%\r\n\r\n\r\n82.2%\r\n\r\n\r\n2020\r\n\r\n\r\n695\r\n\r\n\r\n78.8%\r\n\r\n\r\n83.7%\r\n\r\n\r\nNote: \r\n\r\n\r\n -“Round Winner Accuracy” is defined as the share of rounds in which the predicted round winner matches the winner declared by the majority of judges.\r\n\r\n\r\nFinally, to unpack the model a bit more and see which statistics it relies upon the most to score rounds, we use feature importance. Table 4 below displays the top 10 most important features in the model, where importance is essentially defined as how helpful the feature is in predicting the outcome. Again, when scoring a round, judges are supposed to consider effective striking, effective grappling, octagon control, aggressiveness, and effective defense - in that order. Examining the table below shows that JudgeAI appears to be capturing the exact same criteria in nearly the exact same order. This promising result suggests that both (a) judges generally follow the criteria with which they are provided, as expected, and (b) JudgeAI is able to use the round-level statistics to effectively capture this criteria when predicting round scores.\r\n\r\n\r\nTable 4: Top 10 Most Important Model Features\r\n\r\n\r\nRank\r\n\r\n\r\nFeature\r\n\r\n\r\nCorresponding Judging Criteria\r\n\r\n\r\n1\r\n\r\n\r\nDifference in significant strikes landed\r\n\r\n\r\nEffective striking\r\n\r\n\r\n2\r\n\r\n\r\nDifference in total strikes landed\r\n\r\n\r\nEffective striking\r\n\r\n\r\n3\r\n\r\n\r\nDifference in control time\r\n\r\n\r\nOctagon control, effective grappling\r\n\r\n\r\n4\r\n\r\n\r\nDifference in significant strikes landed to the head\r\n\r\n\r\nEffective striking\r\n\r\n\r\n5\r\n\r\n\r\nDifference in total strikes attempted\r\n\r\n\r\nAggressiveness\r\n\r\n\r\n6\r\n\r\n\r\nDifference in significant strikes attempted\r\n\r\n\r\nAggressiveness\r\n\r\n\r\n7\r\n\r\n\r\nDifference in significant strike accuracy\r\n\r\n\r\nEffective striking, effective defense\r\n\r\n\r\n8\r\n\r\n\r\nDifference in significant strike accuracy to the head\r\n\r\n\r\nEffective striking, effective defense\r\n\r\n\r\n9\r\n\r\n\r\nDifference in significant strikes attempted on the ground\r\n\r\n\r\nEffective grappling\r\n\r\n\r\n10\r\n\r\n\r\nDifference in total strike accuracy\r\n\r\n\r\nEffective striking, effective defense\r\n\r\n\r\nNote: \r\n\r\n\r\n -Feature importance is computed on a model including all rounds in the period, as opposed to using one of the models created via time series cross validation.-Differences are computed by subtracting each blue corner statistic from the corresponding red corner statistic.\r\n\r\n\r\nLimitations of JudgeAI\r\nI’d like to clearly state that JudgeAI is not perfect. I am not claiming that it is better than any particular judge, nor am I suggesting that it should replace judges in the UFC.\r\nWhile the consistently strong performance of JudgeAI over time is promising, this algorithm does have serious limitations. First and foremost, statistics do not perfectly convey damage dealt. Effective striking is the primary judging criteria, and even though accumulating many more significant strikes than one’s opponent does correlate with dealing more damage, just looking at statistical differences will not replace what we can see from watching a fight. This issue is especially problematic in rounds that are close statistically but not as close when you watch it because one fighter clearly lands harder blows.\r\nFurther, JudgeAI seeks to uncover longer-term trends in the relationship between round scores and the features included in the model. Therefore, JudgeAI uses these broader relationships to score every round no matter what, but in reality, many rounds include some sort of rare occurrence, like a devastating calf kick that leaves a fighter largely immobile, that can and perhaps should sway the judges but will not be considered by the model. Similarly, the model only sees the aggregate statistics for the round, so if a particular fighter starts slow in the first half but dominates the second half of a round, the judges might give this round to the strong finisher while the model might not.\r\nFinally, the vast majority of rounds are scored 10-9 for the winner. As a result, JudgeAI struggles to predict 10-8 rounds with a high probability, even when the striking statistics are wildly lopsided. There are methodological adjustments that could be made to increase the model’s precision in this regard, but I have not yet explored such changes.\r\nExtending JudgeAI\r\nThe most obvious extension of JudgeAI would be aggregating the round-level predictions to produce a distribution of potential final scores. Since JudgeAI produces round score probabilities instead of just round win probabilities, it is possible to use the model’s predictions to generate a complete probability distribution for every possible final score. This would allow us to account for the underlying uncertainty in the round-level scoring while identifying the most likely final score.\r\nWhile JudgeAI cannot be used to definitively say that a particular judge gave the wrong score for a particular round, it can be used to assess judges over a larger sample of fights. Since the model is calibrated to reflect the majority decision of judges and since it is largely successful in doing so over time, identifying any judges whose scores regularly deviate from JudgeAI’s scores over a large sample of fights could be fruitful.\r\nFinally and most importantly, I believe JudgeAI can be leveraged to create brand new advanced metrics for MMA. Baseball has expected runs that uses statistics from within an inning to essentially compute the number of runs a team would expect to score on average after generating those statistics. Likewise, soccer has expected goals that uses the probability of scoring on each shot a team takes over the course of a game to essentially calculate the total number of goals a team would expect to score on average after taking shots from those locations. I believe JudgeAI’s predicted round win probabilities can be used to create expected rounds, a metric that measures the number of rounds fighters would expect to win on average after compiling their observed round-level statistics. If successful, this new metric can move the sport away from looking at win percentages or finish percentages and towards a more informative metric that describes how dominant each fighter is on a round-by-round basis.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-19-introducing-judgeai/introducing-judgeai_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-02-22T22:11:18-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
